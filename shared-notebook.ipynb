{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa2b4de",
   "metadata": {},
   "source": [
    "# COM3029 Coursework 2 - Group 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bcaf4a",
   "metadata": {},
   "source": [
    "## Research of Model Serving Options\n",
    "\n",
    "Should this be in the report instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644389ec",
   "metadata": {},
   "source": [
    "## Web Service\n",
    "\n",
    "Show the basic web server script here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552b95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0746adc",
   "metadata": {},
   "source": [
    "## Endpoint Testing\n",
    "\n",
    "Run the unit tests here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d03564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def send_prediction_request(string_to_predict):\n",
    "    return requests.post('http://127.0.0.1:5000/', json={'comment': string_to_predict}).text\n",
    "\n",
    "\n",
    "print(send_prediction_request('test comment 1'))\n",
    "print(send_prediction_request('test comment 2'))\n",
    "\n",
    "r = requests.get('http://127.0.0.1:5000/')\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdba2ed8",
   "metadata": {},
   "source": [
    "### Model Accuracy\n",
    "\n",
    "This test uses 50 comments from the GoEmotions testing dataset to estimate the accuracy of the model. The transformer model used in our web service should achieve an accuracy of around 60-65%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ace0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = datasets.load_dataset(\"go_emotions\")\n",
    "\n",
    "# Maps all unchosen labels to their most similar counterpart.\n",
    "label_mappings = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'admiration', 'optimism', 'curiosity',\n",
    "    'curiosity', 'optimism', 'sadness', 'disapproval', 'annoyance', 'sadness', 'joy',\n",
    "    'sadness', 'gratitude', 'sadness', 'joy', 'love', 'sadness', 'optimism',\n",
    "    'admiration', 'admiration', 'gratitude', 'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "\n",
    "count = 50\n",
    "correct = 0\n",
    "for i in tqdm(range(count)):\n",
    "    choice = random.choice(dataset['test'])\n",
    "    prediction = requests.post('http://127.0.0.1:5000/', json={'comment': choice['text']}).json()['prediction']\n",
    "    \n",
    "    if prediction in [label_mappings[x] for x in choice['labels']]:\n",
    "        correct += 1\n",
    "        \n",
    "print(\"Correct: \" + str(correct) + \"    Incorrect: \" + str(count - correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f28bfd",
   "metadata": {},
   "source": [
    "## Service Performance & Stress Testing\n",
    "\n",
    "The stress testing script is run from the command line once the web service is running, as shown here:\n",
    "\n",
    "`python stress_test.py -u 50 -r 5 -l 30`\n",
    "\n",
    "The contents of this file is shown below for reference:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e369c2",
   "metadata": {},
   "source": [
    "```python\n",
    "import gevent\n",
    "from locust import HttpUser, task, events\n",
    "from locust.env import Environment\n",
    "from locust.stats import stats_printer, stats_history, StatsCSVFileWriter\n",
    "from locust.log import setup_logging\n",
    "\n",
    "from locustfile import StressTest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"-u\", \"--user_count\", dest=\"user_count\", help=\"How many users to be created during a spawn\", type=int, default=1)\n",
    "    parser.add_argument(\"-r\", \"--spawn_rate\", dest=\"spawn_rate\", help=\"How many groups of users should be created every second\", type=int, default=1)\n",
    "    parser.add_argument(\"-l\", \"--length\", dest=\"duration\", help=\"How long the stress test should last for\", type=int, default=30)\n",
    "\n",
    "    stress_args = parser.parse_args()\n",
    "\n",
    "\n",
    "    setup_logging(\"INFO\", None)\n",
    "\n",
    "\n",
    "    # setup Environment and Runner\n",
    "    env = Environment(user_classes=[StressTest], events=events)\n",
    "    runner = env.create_local_runner()\n",
    "\n",
    "    # start a WebUI instance\n",
    "    web_ui = env.create_web_ui(\"127.0.0.1\", 8089)\n",
    "\n",
    "    logging = StatsCSVFileWriter(environment=env, base_filepath='./', full_history=True, percentiles_to_report=[90.0])\n",
    "\n",
    "    # execute init event handlers (only really needed if you have registered any)\n",
    "    env.events.init.fire(environment=env, runner=runner, web_ui=web_ui)\n",
    "\n",
    "    # start a greenlet that periodically outputs the current stats\n",
    "    gevent.spawn(stats_printer(env.stats))\n",
    "\n",
    "    gevent.spawn(logging)\n",
    "\n",
    "    # start a greenlet that save current stats to history\n",
    "    gevent.spawn(stats_history, env.runner)\n",
    "\n",
    "    # start the test\n",
    "    runner.start(user_count=stress_args.user_count, spawn_rate=stress_args.spawn_rate)\n",
    "\n",
    "    # in duration seconds stop the runner\n",
    "    gevent.spawn_later(stress_args.duration, lambda: runner.quit())\n",
    "\n",
    "    # wait for the greenlets\n",
    "    runner.greenlet.join()\n",
    "\n",
    "    # stop the web server for good measures\n",
    "    web_ui.stop()\n",
    "\n",
    "    graph_stats = pd.read_csv('_stats_history.csv')\n",
    "    graph_stats = graph_stats[graph_stats['Name'] == 'Aggregated']\n",
    "    x_axis = graph_stats['Timestamp']\n",
    "    x_axis = x_axis - x_axis.min()\n",
    "\n",
    "    target_metrics = ['Requests/s','Failures/s','Total Request Count','Total Failure Count','Total Average Response Time']\n",
    "\n",
    "    for metric in target_metrics:\n",
    "        y_axis = graph_stats[metric].astype(float)\n",
    "\n",
    "        plt.plot(list(x_axis.values), list(y_axis.values))\n",
    "        plt.title(metric)\n",
    "        save_name = metric.replace('/', '_per_')\n",
    "        plt.savefig(f'{save_name}.png')\n",
    "        plt.clf()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f2f16",
   "metadata": {},
   "source": [
    "## Monitoring Capabilities\n",
    "\n",
    "In order to monitor the user input and predictions of the webservice, the server uses Python's `logging` library to log to a file. For every prediction request made by each user, the time, input text, and predicted label is written as JSON to the `predictions.txt` log file, an example snippet of which is shown here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae9cbd2",
   "metadata": {},
   "source": [
    "```json\n",
    "{\"time\": \"2023-05-23 00:58:45:748136\", \"text\": \"literally I feel like crying\", \"prediction\": \"sadness\"}\n",
    "{\"time\": \"2023-05-23 00:58:46:135930\", \"text\": \"i love this subreddit\", \"prediction\": \"love\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4969991f",
   "metadata": {},
   "source": [
    "JSON is used so that the log file can be parsed programatically and used by a different service, if if ever becomes necessary. This structured logging is implemented using a custom class to store all the necessary data. There is also a class to log error messages, for when the user sends invalid text. Both of these are shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801fba6",
   "metadata": {},
   "source": [
    "```python\n",
    "class LogMsg(object):\n",
    "    def __init__(self, text, prediction):\n",
    "        self.text = text\n",
    "        self.prediction = prediction\n",
    "        self.time = datetime.now().strftime('%Y-%m-%d %H:%M:%S:%f')\n",
    "\n",
    "    def __str__(self):\n",
    "        return json.dumps({'time': self.time, 'text': self.text, 'prediction': self.prediction})\n",
    "    \n",
    "class LogMsgInputFormat400Error(object):\n",
    "    def __init__(self):\n",
    "        self.time = datetime.now().strftime('%Y-%m-%d %H:%M:%S:%f')\n",
    "\n",
    "    def __str__(self):\n",
    "        return json.dumps({'time': self.time, 'error': \"The message must be JSON in the form json={'comment': string_to_predict}.\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88edfce4",
   "metadata": {},
   "source": [
    "Logging a regular request and an invalid request are then done like so:\n",
    "\n",
    "```python\n",
    "# Invalid request:\n",
    "app.logger.info(LogMsgInputFormat400Error())\n",
    "\n",
    "# Valid request:\n",
    "app.logger.info(LogMsg(comment_text, prediction))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db164eb9",
   "metadata": {},
   "source": [
    "## CI/CD Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc8eb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
